{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facfa308",
   "metadata": {},
   "source": [
    "The following notebook generates \"startup names\" by reading a dataset of\n",
    "public companies.\n",
    "\n",
    "Sources:\n",
    "[TextKD-GAN: Text Generation using KnowledgeDistillation and Generative Adversarial Networks](https://arxiv.org/abs/1905.01976)\n",
    "[Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028v1)\n",
    "[\"NASDAQ and NYSE stocks histories\" on Kaggle](https://www.kaggle.com/qks1lver/nasdaq-and-nyse-stocks-histories?select=NASDAQ.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832731ed-7509-4a09-81a6-0f1ec95f4745",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "832731ed-7509-4a09-81a6-0f1ec95f4745",
    "outputId": "cb744b0e-3dd1-4bd6-a741-0abef43883d4"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy tensorflow tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea99c3-4163-40c2-99ad-2fe50b809a17",
   "metadata": {
    "id": "3fea99c3-4163-40c2-99ad-2fe50b809a17"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Read in the dataframe.\n",
    "df = pandas.read_csv('./stocks.txt', dtype=str, keep_default_na=False, sep='\\t',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3da57d-1b66-4c26-ac1d-1d90f068cb54",
   "metadata": {
    "id": "ac3da57d-1b66-4c26-ac1d-1d90f068cb54"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "# The location for the new vocabulary.\n",
    "vocab_location = 'vocab-' + str(int(time.time())) + '.txt'\n",
    "\n",
    "# Reserved tokens to include in the vocabulary. The \"[padding]\" token is used \n",
    "# later to pad short names to be the same width.\n",
    "reserved_tokens = ['[padding]']\n",
    "\n",
    "# The number of vocabulary tokens to generate.\n",
    "vocab_size = 300\n",
    "\n",
    "def get_vocab():\n",
    "    vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        tf.data.Dataset.from_tensor_slices(df['Company']), \n",
    "        vocab_size=vocab_size, \n",
    "        reserved_tokens=reserved_tokens, \n",
    "        bert_tokenizer_params={}\n",
    "    )\n",
    "    return vocab\n",
    "\n",
    "# Load the vocabulary and write it to a file.\n",
    "vocab = get_vocab()\n",
    "with open(vocab_location, 'w') as file:\n",
    "    for token in vocab:\n",
    "        print(token, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e5bb2-dda3-4da3-b01b-af8772b6e79c",
   "metadata": {
    "id": "273e5bb2-dda3-4da3-b01b-af8772b6e79c"
   },
   "outputs": [],
   "source": [
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "# The number of tokens expected for each name. Longer names will be \n",
    "# truncated, and shorter ones will be padded.\n",
    "input_shape = 20\n",
    "\n",
    "# The index of the padding token in the vocabulary.\n",
    "padding_token = 0\n",
    "\n",
    "string_lookup = text.BertTokenizer(vocab_location)\n",
    "\n",
    "def encode_name(name):\n",
    "    tokenized = string_lookup.tokenize(name).merge_dims(-2, -1)[0]\n",
    "    padded = tf.slice(\n",
    "        tf.pad(\n",
    "            tokenized, \n",
    "            [[0, max(0, input_shape - len(tokenized))]], \n",
    "            constant_values=padding_token\n",
    "        ),\n",
    "        [0],\n",
    "        [input_shape]\n",
    "    )\n",
    "\n",
    "    return tf.one_hot(padded, 300)\n",
    "\n",
    "def without_short_words(name):\n",
    "    return ' '.join([word for word in name.split(' ') if len(word) >= 4])\n",
    "\n",
    "def get_names_encoded():\n",
    "    return tf.convert_to_tensor([\n",
    "        encode_name(without_short_words(company)) for company in df['Company']\n",
    "    ])\n",
    "\n",
    "# Load the names as one-hot vectors.\n",
    "one_hot_names = get_names_encoded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714679e-ac4b-4483-a209-eb9e0d6de8ce",
   "metadata": {
    "id": "3714679e-ac4b-4483-a209-eb9e0d6de8ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "conv_dim = 100\n",
    "generator_noise_shape = (100,)\n",
    "\n",
    "def get_autoencoder():\n",
    "    # An autoencoder is used to soften real inputs to the discriminator.\n",
    "    # See \"TextKD-GAN: Text Generation using KnowledgeDistillation and \n",
    "    # Generative Adversarial Networks.\"\n",
    "    return keras.Sequential([\n",
    "        keras.Input((input_shape, vocab_size), name=\"autoencoder_input\"),\n",
    "        layers.Flatten(name=\"autoencoder_1\"),\n",
    "        layers.Dense(4, name=\"autoencoder_2\"),\n",
    "        layers.Dense(input_shape * vocab_size, name=\"autoencoder_3\"),\n",
    "        layers.Reshape((input_shape, vocab_size), name=\"autoencoder_4\"),\n",
    "        layers.Softmax(name=\"autoencoder_5\")\n",
    "    ])\n",
    "\n",
    "def res_block():\n",
    "    # Each residual block within the residual network modifies\n",
    "    # the input. The convolutional layer uses the padding option\n",
    "    # \"same\" to preserve the same dimensions as the input in the output,\n",
    "    # so that addition can be performed at the end. This ensures\n",
    "    # the input and output have size (input_shape, conv_dim).\n",
    "    inputs = keras.Input((input_shape, conv_dim))\n",
    "    x = inputs\n",
    "    x = layers.Conv1D(conv_dim, 5, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv1D(conv_dim, 5, activation='relu', padding='same')(x)\n",
    "    x = inputs + x * 0.3\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def get_discriminator():\n",
    "    # The discriminator takes size (input_shape, vocab_size) and\n",
    "    # outputs size (1).\n",
    "    return keras.Sequential([\n",
    "        keras.Input((input_shape, vocab_size), name=\"discriminator_input\"),\n",
    "        layers.Conv1D(conv_dim, 1, activation='relu'),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, name=\"discriminator_4\"),\n",
    "    ])\n",
    "\n",
    "def get_generator():\n",
    "    # The generator takes size (generator_noise_shape) and\n",
    "    # outputs size (input_shape, vocab_size).\n",
    "    return keras.Sequential([\n",
    "        keras.Input(generator_noise_shape, name=\"generator_input\"),\n",
    "        layers.Dense(input_shape * conv_dim),\n",
    "        layers.Reshape((input_shape, conv_dim)),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        res_block(),\n",
    "        layers.Conv1D(vocab_size, 1, activation='relu'),\n",
    "        layers.Softmax(name=\"generator_6\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa24b1-aa98-4006-868a-907f66af4592",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "2ffa24b1-aa98-4006-868a-907f66af4592",
    "outputId": "c4d618f2-efd8-49d6-f7bb-077941fe2cbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "import os\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "# The number of training iterations to run.\n",
    "training_iterations = 5\n",
    "\n",
    "# The number of epochs to run for the autoencoder\n",
    "# within each iteration.\n",
    "autoencoder_epochs = 10\n",
    "\n",
    "# The number of epochs to run for the discriminator\n",
    "# and generator within each iteration.\n",
    "discriminator_training_iterations = 500\n",
    "\n",
    "# The batch size to use when training the discriminator\n",
    "# and generator.\n",
    "discriminator_batch_size = 512\n",
    "\n",
    "# The shuffle buffer size to use when training\n",
    "# the discriminator and generator, which can be set\n",
    "# to the number of real names to ensure a complete shuffle.\n",
    "discriminator_shuffle_buffer_size = len(one_hot_names)\n",
    "\n",
    "# The coefficient to use in the gradient penalty for\n",
    "# enforcing the Lipschitz constraint. See\n",
    "# \"Improved Training of Wasserstein GANs.\"\n",
    "lipschitz_lambda = 10\n",
    "\n",
    "autoencoder = get_autoencoder()\n",
    "discriminator = get_discriminator()\n",
    "generator = get_generator()\n",
    "\n",
    "one_hot_named_batched_shuffled = tf.data.Dataset \\\n",
    "    .from_tensor_slices(one_hot_names) \\\n",
    "    .shuffle(discriminator_shuffle_buffer_size, reshuffle_each_iteration=True) \\\n",
    "    .batch(discriminator_batch_size, drop_remainder=True)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4, \n",
    "    beta_1=0.5, \n",
    "    beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(\n",
    "    1e-4, \n",
    "    beta_1=0.5, \n",
    "    beta_2=0.9\n",
    ")\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    print('Iteration {}/{}...'.format(i + 1, training_iterations))\n",
    "\n",
    "    # Train the autoencoder using Keras's `fit` method.\n",
    "    autoencoder_history = autoencoder.fit(\n",
    "        one_hot_names,\n",
    "        one_hot_names, \n",
    "        epochs=autoencoder_epochs, \n",
    "        verbose=0, \n",
    "        shuffle=True, \n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        '  Encode:\\n    training loss: mean {}, stdev {}\\n    validation loss: mean {}, stdev {}'.format(\n",
    "            statistics.mean(autoencoder_history.history['loss']),\n",
    "            statistics.stdev(autoencoder_history.history['loss']),\n",
    "            statistics.mean(autoencoder_history.history['val_loss']),\n",
    "            statistics.stdev(autoencoder_history.history['val_loss'])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for j in range(discriminator_training_iterations):\n",
    "        k = 0\n",
    "        for discriminator_batch in one_hot_named_batched_shuffled:\n",
    "            k += 1\n",
    "\n",
    "            noise = tf.random.normal(\n",
    "                [discriminator_batch_size] + list(generator_noise_shape)\n",
    "            )\n",
    "\n",
    "            # Compute gradients for the generator and discriminator for this batch.\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as lipschitz_tape:\n",
    "                generated = generator(noise, training=True)\n",
    "\n",
    "                real_discriminated = discriminator((discriminator_batch), training=True)\n",
    "                generated_discriminated = discriminator(generated, training=True)\n",
    "\n",
    "                # Compute the Wasserstein loss. See \"Improved Training of\n",
    "                # Wasserstein GANs.\"\n",
    "                generator_loss = -tf.reduce_mean(generated_discriminated)\n",
    "                discriminator_loss_initial = tf.reduce_mean(generated_discriminated) - tf.reduce_mean(real_discriminated)\n",
    "\n",
    "                # Compute the gradient penalty to enforce the Lipschitz \n",
    "                # constraint. See \"Improved Training of Wasserstein GANs.\"\n",
    "                lipschitz_line_alpha = tf.random.uniform(\n",
    "                    shape=[discriminator_batch_size, 1, 1], \n",
    "                    minval=0,\n",
    "                    maxval=1\n",
    "                )\n",
    "                lipschitz_line = discriminator_batch + lipschitz_line_alpha * (discriminator_batch - generated)\n",
    "                lipschitz_discriminated = discriminator(\n",
    "                    lipschitz_line, training=True\n",
    "                )\n",
    "                gradients_of_lipschitz = lipschitz_tape.gradient(\n",
    "                    lipschitz_discriminated, [lipschitz_line]\n",
    "                )[0]\n",
    "                lipschitz_slopes_by_batch = tf.math.sqrt(\n",
    "                    tf.math.reduce_sum(tf.math.square(gradients_of_lipschitz), \n",
    "                    axis=[-2, -1])\n",
    "                )\n",
    "                lipschitz_mean = tf.math.reduce_mean(\n",
    "                    tf.math.square(lipschitz_slopes_by_batch - 1)\n",
    "                )\n",
    "\n",
    "                discriminator_loss = discriminator_loss_initial + lipschitz_lambda * lipschitz_mean\n",
    "\n",
    "            gradients_of_generator = gen_tape.gradient(\n",
    "                generator_loss, \n",
    "                generator.trainable_variables\n",
    "            )            \n",
    "            gradients_of_discriminator = disc_tape.gradient(\n",
    "                discriminator_loss, \n",
    "                discriminator.trainable_variables\n",
    "            )\n",
    "\n",
    "            # Apply the gradients to the generator and discriminator.\n",
    "            generator_optimizer.apply_gradients(zip(\n",
    "                gradients_of_generator, \n",
    "                generator.trainable_variables\n",
    "            ))\n",
    "            discriminator_optimizer.apply_gradients(zip(\n",
    "                gradients_of_discriminator, \n",
    "                discriminator.trainable_variables\n",
    "            ))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        estimated_total = elapsed / (j + 1 + i * discriminator_training_iterations) * discriminator_training_iterations * training_iterations\n",
    "        estimated_remaining = estimated_total - elapsed\n",
    "\n",
    "        print(\n",
    "            '  Discriminator: Epoch {}/{}. Generator loss {}. Discriminator loss {}. Total time {}, estimated {} remaining.'.format(\n",
    "                j + 1, \n",
    "                discriminator_training_iterations, \n",
    "                generator_loss.numpy(), \n",
    "                discriminator_loss.numpy(),\n",
    "                str(timedelta(seconds=elapsed)).split('.')[0],\n",
    "                str(timedelta(seconds=estimated_remaining)).split('.')[0]\n",
    "            )\n",
    "        )\n",
    "\n",
    "def save_models():\n",
    "    suffix = str(int(time.time()))\n",
    "    os.makedirs('models/' + suffix)\n",
    "    autoencoder.save('./models/' + suffix + '/autoencoder')\n",
    "    generator.save('./models/' + suffix + '/generator')\n",
    "    discriminator.save('./models/' + suffix + '/discriminator')\n",
    "\n",
    "save_models()\n",
    "\n",
    "print('Done training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d5f14-c392-4ec0-8e97-a954398dd79d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a4d5f14-c392-4ec0-8e97-a954398dd79d",
    "outputId": "e9ae84b1-ff3a-4c33-994c-4bec53c32663",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The number of names to generate.\n",
    "random_iterations = 5\n",
    "\n",
    "# How many names are generated for each one\n",
    "# shown. Setting this to a value higher than 1\n",
    "# will lead to the best name (as scored by the\n",
    "# discriminator) being shown from each group.\n",
    "compared_generated_names = 1\n",
    "\n",
    "def simplify_name(name):\n",
    "    new_name = name\n",
    "\n",
    "    # Remove reserved tokens such as \"[padding]\" from\n",
    "    # the string.\n",
    "    for token in reserved_tokens:\n",
    "        new_name = new_name.replace(token, '')\n",
    "\n",
    "    # Remove words with fewer than three characters.\n",
    "    return ' '.join([word for word in new_name.split(' ') if len(word) >= 3])\n",
    "\n",
    "def decode_names(encoded_names):\n",
    "    max_indices = tf.math.argmax(encoded_names, axis=-1)\n",
    "    joined = tf.strings.reduce_join(\n",
    "        string_lookup.detokenize(max_indices), \n",
    "        axis=-1, \n",
    "        separator=' '\n",
    "    )\n",
    "    return [simplify_name(name.numpy().decode('utf-8')) for name in joined]\n",
    "\n",
    "def is_valid_name(encoded_name):\n",
    "    # Names that include \"##\" indicate that a suffix\n",
    "    # from the BERT vocabulary was placed at the\n",
    "    # beginning of a word, and therefore they \n",
    "    # should be excluded.\n",
    "    return '##' not in decode_names([encoded_name])[0]\n",
    "\n",
    "for i in range(random_iterations):\n",
    "    # Generate and discriminate new names.\n",
    "    generated_options = generator(\n",
    "        tf.random.normal([compared_generated_names] + list(generator_noise_shape))\n",
    "    )\n",
    "    discriminated_options = discriminator(generated_options)\n",
    "\n",
    "    # Determine the best generated name, if more than one\n",
    "    # were generated.\n",
    "    best_generated_index = 0\n",
    "    best_discriminated = 0\n",
    "    for i in range(len(generated_options)):\n",
    "        if discriminated_options[i] > best_discriminated and is_valid_name(generated_options[i]):\n",
    "            best_discriminated = discriminated_options[i]\n",
    "            best_generated_index = i\n",
    "\n",
    "    generated_choice = [generated_options[best_generated_index]]\n",
    "    discriminated_choice = [discriminated_options[best_generated_index]]\n",
    "    inverted_join = decode_names(generated_choice)\n",
    "\n",
    "    # Randomly choose a real name and score it\n",
    "    # using the discriminator.\n",
    "    element_real = [random.choice(one_hot_names)]\n",
    "    discriminated_real = discriminator((tf.convert_to_tensor(element_real)))\n",
    "    inverted_join_real = decode_names(element_real)\n",
    "\n",
    "    # Print the generated and real names along with their scores.\n",
    "    print('\\n'.join([\n",
    "        word + '\\t' + str(confidence.numpy()) + '\\t\\t' + real_word + '\\t' + str(real_confidence.numpy()) \\\n",
    "            for word, confidence, real_word, real_confidence \\\n",
    "            in zip(\n",
    "                inverted_join, \n",
    "                discriminated_choice, \n",
    "                inverted_join_real, \n",
    "                discriminated_real\n",
    "            )\n",
    "    ]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "names-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
